import subprocess
import sys
import os
import asyncio
import torch
import nest_asyncio

# --- 1. –£–°–¢–ê–ù–û–í–ö–ê –ë–ò–ë–õ–ò–û–¢–ï–ö ---
def install_packages():
    try:
        import aiogram
    except ImportError:
        packages = ["aiogram", "ultralytics", "transformers", "accelerate", "nest_asyncio"]
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-U"] + packages)

install_packages()

from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command
from ultralytics import YOLO
from transformers import AutoModelForCausalLM, AutoTokenizer

nest_asyncio.apply()

# --- 2. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
CONFIG = {
    "TOKEN": "–¢–£–¢_–ú–û–ì_–ë–´–¢–¨_–¢–û–ö–ï–ù",
    "CV_MODEL_PATH": "best_skin_model.pt",
    "LLM_NAME": "Qwen/Qwen2.5-3B-Instruct", 
    "DISEASE_RU": {
        'acne': '–∞–∫–Ω–µ', 'eksim': '—ç–∫–∑–µ–º–∞', 'herpes': '–≥–µ—Ä–ø–µ—Å', 
        'panu': '–æ—Ç—Ä—É–±–µ–≤–∏–¥–Ω—ã–π –ª–∏—à–∞–π', 'rosacea': '—Ä–æ–∑–∞—Ü–µ–∞'
    },
    "SYSTEM_PROMPT": (
        "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ—Å–º–µ—Ç–∏—á–µ—Å–∫–æ–º—É —É—Ö–æ–¥—É –∑–∞ –∫–æ–∂–µ–π. –¢–≤–æ—è —Ä–æ–ª—å: –¥–∞–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ "
        "–ø–æ –æ—á–∏—â–µ–Ω–∏—é, —É–≤–ª–∞–∂–Ω–µ–Ω–∏—é –∏ SPF. –ó–∞–ø—Ä–µ—â–µ–Ω–æ —Å—Ç–∞–≤–∏—Ç—å –¥–∏–∞–≥–Ω–æ–∑—ã –∏ –Ω–∞–∑–Ω–∞—á–∞—Ç—å –ª–µ—á–µ–Ω–∏–µ. "
        "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø—É–Ω–∫—Ç–∞–º–∏, –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
    )
}

# --- 3. –ò–ò –°–ò–°–¢–ï–ú–ê ---
class SkinCareAI:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ CV
        if os.path.exists(CONFIG["CV_MODEL_PATH"]):
            self.cv_model = YOLO(CONFIG["CV_MODEL_PATH"])
            self.cv_available = True
            print("‚úÖ YOLOv8 —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        else:
            self.cv_model = None
            self.cv_available = False
            print("‚ö†Ô∏è –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –†–∞–±–æ—Ç–∞ –≤ —Ä–µ–∂–∏–º–µ —á–∞—Ç–∞.")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ Qwen 2.5
        print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ {CONFIG['LLM_NAME']}...")
        self.tokenizer = AutoTokenizer.from_pretrained(CONFIG["LLM_NAME"])
        self.model = AutoModelForCausalLM.from_pretrained(
            CONFIG["LLM_NAME"],
            dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print("‚úÖ Qwen –≥–æ—Ç–æ–≤–∞.")

    def generate_advice(self, user_text, is_analysis=False):
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"""
        prompt_text = f"–£ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ: {user_text}. –î–∞–π –∫–æ—Å–º–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã." if is_analysis else user_text
        
        messages = [
            {"role": "system", "content": CONFIG["SYSTEM_PROMPT"]},
            {"role": "user", "content": prompt_text}
        ]
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ —Ç–µ–≥–∞–º–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer([formatted_prompt], return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=400,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¢–û–õ–¨–ö–û –Ω–æ–≤—ã–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        in_len = inputs.input_ids.shape[1]
        response_ids = generated_ids[0][in_len:]
        return self.tokenizer.decode(response_ids, skip_special_tokens=True).strip()

    def process_image(self, img_path):
        if not self.cv_available: return None, 0, "–ú–æ–¥—É–ª—å –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–∫–ª—é—á–µ–Ω."
        
        results = self.cv_model.predict(img_path, conf=0.2, verbose=False)
        result = results[0] # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É 'list'

        if result.probs is None:
            return None, 0, "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ."

        top_idx = result.probs.top1
        class_name = result.names[top_idx]
        conf = result.probs.top1conf.item()
        disease_ru = CONFIG["DISEASE_RU"].get(class_name, "–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–æ–∂–∏")
        
        advice = self.generate_advice(disease_ru, is_analysis=True)
        return disease_ru, conf, advice

# --- 4. –¢–ï–õ–ï–ì–†–ê–ú –ë–û–¢ ---
ai_system = SkinCareAI()
bot = Bot(token=CONFIG["TOKEN"])
dp = Dispatcher()

@dp.message(Command("start"))
async def cmd_start(message: types.Message):
    mode = "üì∏ –ü—Ä–∏—à–ª–∏ —Ñ–æ—Ç–æ –∏–ª–∏ –Ω–∞–ø–∏—à–∏ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º." if ai_system.cv_available else "üìù –ó–∞–¥–∞–π –≤–æ–ø—Ä–æ—Å –æ–± —É—Ö–æ–¥–µ –∑–∞ –∫–æ–∂–µ–π —Ç–µ–∫—Å—Ç–æ–º."
    await message.answer(f"–ü—Ä–∏–≤–µ—Ç! –Ø –ò–ò-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç 2025.\n\n{mode}")

@dp.message(F.photo)
async def handle_photo(message: types.Message):
    if not ai_system.cv_available:
        return await message.reply("–ú–æ–¥—É–ª—å —Ñ–æ—Ç–æ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º.")

    status = await message.answer("‚åõ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ñ–æ—Ç–æ –∏ –≥–æ—Ç–æ–≤–ª—é —Å–æ–≤–µ—Ç—ã...")
    path = f"img_{message.from_user.id}.jpg"
    await bot.download_file((await bot.get_file(message.photo[-1].file_id)).file_path, path)

    try:
        res, conf, adv = await asyncio.to_thread(ai_system.process_image, path)
        if res:
            text = f"<b>–í–∏–∑—É–∞–ª—å–Ω–æ:</b> {res}\n<b>–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:</b> {conf:.1%}\n\n<b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</b>\n{adv}"
            await message.reply(text, parse_mode="HTML")
        else:
            await message.reply(adv)
    except Exception as e:
        await message.reply(f"–û—à–∏–±–∫–∞: {e}")
    finally:
        await status.delete()
        if os.path.exists(path): os.remove(path)

@dp.message(F.text)
async def handle_text(message: types.Message):
    status = await message.answer("ü§î –î—É–º–∞—é –Ω–∞–¥ –æ—Ç–≤–µ—Ç–æ–º...")
    try:
        reply = await asyncio.to_thread(ai_system.generate_advice, message.text)
        await message.reply(reply if reply else "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")
    finally:
        await status.delete()

async def main():
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω! –ú–æ–∂–Ω–æ –ø–∏—Å–∞—Ç—å –≤ Telegram.")
    await dp.start_polling(bot)

if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    loop.create_task(main())
